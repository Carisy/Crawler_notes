# scrapy  

## 利用scrapy shell网站解析测试（JPHP306）

1.scrapy shll 要解析的网址 --nolog
2.进入>>>下
3.ti = sel.xpath("/html/head/title")
  print(ti)
  
  
  
## 杂
spider.py中的parse方法是处理scrapy爬虫爬行到网页响应（response）的默认方法

## robots.txt
scrapy默认是开着的，写之前要在settings.py中将它注释掉
即
# Obey robots.txt rules
# ROBOTSTXT_OBEY = True


##用户代理池
（1）在settings中设置用户代理池
# 用户代理（User-Agent）池设置
UAPOOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
    "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
]


（2）创建下载中间文件uamid.py（与settings.py同一个目录），或者直接在middlewares.py中，
导入库
from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware
from douban.settings import UAPOOL
import random

创建类
# 用户代理池
class Uamid(UserAgentMiddleware):
    def __init__(self, user_agent=""):
        self.user_agent = user_agent

    def process_request(self, request, spider):
        thisua = random.choice(UAPOOL)
        print("当前使用的User-Agent是： " + thisua)
        request.headers.setdefault("User-Agent", thisua)

（3）在settings文件中配置中间下载件（注意优先级）
DOWNLOADER_MIDDLEWARES = {
    # 'douban.middlewares.MyCustomDownloaderMiddleware': 543,
    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 2,
    'douban.middlewares.Uamid': 1
}


##关于请求头（cookies和User-Agent另有配置，没写）（最好通过查看浏览器浏览者以网页的请求报头进行模拟）
在setting文件中
# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'en',
    'Cache-Control': 'max-age=0',
    'Connection': 'keep-alive',
    'Host': 'read.douban.com',
    'Referer': 'www.baidu.com',        
    # 'referer': 'https://read.douban.com/ebooks/?dcs=book-nav&dcm=douban',
    'Upgrade-Insecure-Requests':' 1',
    #'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'
}


Ctrl + Shift + F 全局查找
Ctrl + Shift + R 全局替换
(.*?):(.*)
'$1':'$2',



###Referer
'Referer': 'www.baidu.com'即从www.baidu.com页面点击链接来这个页面的


###cookies（写爬虫之前一定要考虑）
在scrapy中，
当COOKIES_ENABLED是注释的时候scrapy默认没有开启cookie
当COOKIES_ENABLED没有注释设置为False的时候scrapy默认使用了settings里面的cookie
当COOKIES_ENABLED设置为True的时候scrapy就会把settings的cookie关掉，使用自定义cookie
总结：
如果使用自定义cookie就把COOKIES_ENABLED设置为True
如果使用settings的cookie就把COOKIES_ENABLED设置为False
####使用
如果，网站不需要登录就能爬去信息，可以禁用cookies





##ajax请求
豆瓣爬书https://read.douban.com/category/?kind=100
这个网页的书名，作者等信息都是通过ajax再请求加载出来的。
并且ajax的url为https://read.douban.com/j/kind/,并且请求方式是post


##scrapy post请求
scrapy默认发送的是get请求
发送post请求时需要重载start_requests(self):
