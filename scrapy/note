# scrapy  

## 利用scrapy shell网站解析测试（JPHP306）

1.scrapy shll 要解析的网址 --nolog
2.进入>>>下
3.ti = sel.xpath("/html/head/title")
  print(ti)
  
  
  
## 杂
spider.py中的parse方法是处理scrapy爬虫爬行到网页响应（response）的默认方法

## robots.txt
scrapy默认是开着的，写之前要在settings.py中将它注释掉
即
# Obey robots.txt rules
# ROBOTSTXT_OBEY = True


##用户代理池
（1）在settings中设置用户代理池
# 用户代理（User-Agent）池设置
UAPOOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
    "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
]


（2）创建下载中间文件uamid.py（与settings.py同一个目录），或者直接在middlewares.py中，
导入库
from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware
from douban.settings import UAPOOL
import random

创建类
# 用户代理池
class Uamid(UserAgentMiddleware):
    def __init__(self, user_agent=""):
        self.user_agent = user_agent

    def process_request(self, request, spider):
        thisua = random.choice(UAPOOL)
        print("当前使用的User-Agent是： " + thisua)
        request.headers.setdefault("User-Agent", thisua)

（3）在settings文件中配置中间下载件（注意优先级）
DOWNLOADER_MIDDLEWARES = {
    # 'douban.middlewares.MyCustomDownloaderMiddleware': 543,
    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 2,
    'douban.middlewares.Uamid': 1
}


##关于请求头（cookies和User-Agent另有配置，没写）（最好通过查看浏览器浏览者以网页的请求报头进行模拟）
在setting文件中
# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'en',
    'Cache-Control': 'max-age=0',
    'Connection': 'keep-alive',
    'Host': 'read.douban.com',
    'Referer': 'www.baidu.com',        
    # 'referer': 'https://read.douban.com/ebooks/?dcs=book-nav&dcm=douban',
    'Upgrade-Insecure-Requests':' 1',
    #'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'
}


Ctrl + Shift + F 全局查找
Ctrl + Shift + R 全局替换
(.*?):(.*)
'$1':'$2',



###Referer
'Referer': 'www.baidu.com'即从www.baidu.com页面点击链接来这个页面的


###cookies（写爬虫之前一定要考虑）
在scrapy中，
当COOKIES_ENABLED是注释的时候scrapy默认没有开启cookie
当COOKIES_ENABLED没有注释设置为False的时候scrapy默认使用了settings里面的cookie
当COOKIES_ENABLED设置为True的时候scrapy就会把settings的cookie关掉，使用自定义cookie
总结：
如果使用自定义cookie就把COOKIES_ENABLED设置为True
如果使用settings的cookie就把COOKIES_ENABLED设置为False
####使用
如果，网站不需要登录就能爬去信息，可以禁用cookies





##ajax请求
豆瓣爬书https://read.douban.com/category/?kind=100
这个网页的书名，作者等信息都是通过ajax再请求加载出来的。
并且ajax的url为https://read.douban.com/j/kind/,并且请求方式是post


##scrapy post请求
scrapy默认发送的是get请求
发送post请求时需要重载start_requests(self):


##关于解析（P3P482）
一次解析：
some_books = response.xpath('//li[@class="subject-item"]')
二次解析：
加点在li节点内部提取：item['book_name'] = book.xpath('.//h2[@class=""]/a/@title')
不加点从根节点开始提取：item['book_name'] = book.xpath('//h2[@class=""]/a/@title')


###extract_first()
scrapy在经过几次解析之后，限定了匹配的范围，使XPATH只可以匹配到一个元素，然后用extract（）方法提取，结果是一个列表形式（列表里的元素只有一个，是我们所要的）。
我们将extract_first（）方法替换extract（）方法，来获取列表内容的第一个元素（不再是列表，而是里面的元素）
some_books = response.xpath('//li[@class="subject-item"]')
item['book_name'] = book.xpath('.//h2[@class=""]/a/@title').extract()
//获取的元素是列表形式，是一个只包含一个元素的列表

some_books = response.xpath('//li[@class="subject-item"]')
item['book_name'] = book.xpath('.//h2[@class=""]/a/@title').extract_first()
//返回的元素是这个列表中的第一个元素


###解析时元素碰到空格如何解决
1.join()方法+split()方法，可以去除全部空格
# join为字符字符串合成传入一个字符串列表，split用于字符串分割可以按规则进行分割

>>> a = " a b c "
>>> b = a.split()  # 字符串按空格分割成列表
>>> b ['a', 'b', 'c']
>>> c = "".join(b) # 使用一个空字符串合成列表内容生成新的字符串
>>> c 'abc'

# 快捷用法
>>> a = " a b c "
>>> "".join(a.split())
'abc'
又例：
>>> a = 'hello world'
>>> a = ''.join(a.split())
>>> print(a)
helloworld

2.使用正则表达式
>>> a = 'hello world'
>>> import re
>>> strinfo = re.compile()
>>> strinfo = re.compile(' ')

>>> b = strinfo.sub('', a)
>>> print(b)
helloworld

3.使用replace方法
>>> a = 'hello world'
>>> a.replace(' ', '')  //用‘ ’替换‘’
'helloworld'

##scrapy爬取并请求下一页链接
base_url = 'https://book.douban.com'
part_url = response.xpath('//*[@id="subject_list"]/div[2]/span[4]/a/@href').extract_first() //提取下一页链接关键部分或者可能是完整部分
url = base_url+part_url
yield scrapy.Request(url=url, callback=self.parse())


##解析时使用正则表达式
some_books = response.xpath('//li[@class="subject-item"]')
        for book in some_books:
        item['book_author'] = book.xpath('.//div[@class="pub"]/text()').re_first('\s*(\S.*?)\/')
        
//解析时碰到很多换行符和字符串用\s*

注意：
在scrapy中，
不能直接用.re，如item = book.xpath('.').re('\s*(\S.*?)\/')，需要xpath等解析器来过渡
.re（）功能与xpath等解析器类似
.re_first()功能与.xpath（）.extract_first（）等类似


##返回网址响应时，注意回到函数的参数，没有括号
yield scrapy.Request(url=one_url, callback=self.follow_parse)
