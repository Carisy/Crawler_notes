# scrapy  

##关于settings.py文件一开始的处理
###关于是否遵守robots.txt协议
代码：
# Obey robots.txt rules
ROBOTSTXT_OBEY = True//遵守

# Obey robots.txt rules
# ROBOTSTXT_OBEY = True//不遵守

 
###关于限速
并发与延迟<===
代码：
#1、下载器总共最大处理的并发请求数,默认值16
#CONCURRENT_REQUESTS = 32
#2、每个域名能够被执行的最大并发请求数目，默认值8
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#3、能够被单个IP处理的并发请求数，默认值0，代表无限制，需要注意两点
#I、如果不为零，那CONCURRENT_REQUESTS_PER_DOMAIN将被忽略，即并发数的限制是按照每个IP来计算，而不是每个域名
#II、该设置也影响DOWNLOAD_DELAY，如果该值不为零，那么DOWNLOAD_DELAY下载延迟是限制每个IP而不是每个域
#CONCURRENT_REQUESTS_PER_IP = 16

#4、如果没有开启智能限速，这个值就代表一个规定死的值，代表对同一网址延迟请求的秒数
#下载器在下载同一个网站下一个页面前需要等待的时间,该选项可以用来限制爬取速度,减轻服务器压力。同时也支持小数:0.25 以秒为单位
#DOWNLOAD_DELAY = 3








## 利用scrapy shell网站解析测试（JPHP306）

1.scrapy shll 要解析的网址 --nolog
2.进入>>>下
3.ti = sel.xpath("/html/head/title")
  print(ti)
  
  
  
## 杂
spider.py中的parse方法是处理scrapy爬虫爬行到网页响应（response）的默认方法

## robots.txt
scrapy默认是开着的，写之前要在settings.py中将它注释掉
即
# Obey robots.txt rules
# ROBOTSTXT_OBEY = True


##用户代理池
（1）在settings中设置用户代理池
# 用户代理（User-Agent）池设置
UAPOOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
    "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
]


（2）创建下载中间文件uamid.py（与settings.py同一个目录），或者直接在middlewares.py中，
导入库
from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware
from douban.settings import UAPOOL
import random

创建类
# 用户代理池
class Uamid(UserAgentMiddleware):
    def __init__(self, user_agent=""):
        self.user_agent = user_agent

    def process_request(self, request, spider):
        thisua = random.choice(UAPOOL)
        print("当前使用的User-Agent是： " + thisua)
        request.headers.setdefault("User-Agent", thisua)

（3）在settings文件中配置中间下载件（注意优先级）
DOWNLOADER_MIDDLEWARES = {
    # 'douban.middlewares.MyCustomDownloaderMiddleware': 543,
    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 2,
    'douban.middlewares.Uamid': 1
}


##关于请求头（cookies和User-Agent另有配置，没写）（最好通过查看浏览器浏览者以网页的请求报头进行模拟）
在setting文件中
# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'en',
    'Cache-Control': 'max-age=0',
    'Connection': 'keep-alive',
    'Host': 'read.douban.com',
    'Referer': 'www.baidu.com',        
    # 'referer': 'https://read.douban.com/ebooks/?dcs=book-nav&dcm=douban',
    'Upgrade-Insecure-Requests':' 1',
    #'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'
}


Ctrl + Shift + F 全局查找
Ctrl + Shift + R 全局替换
(.*?):(.*)
'$1':'$2',



###Referer
'Referer': 'www.baidu.com'即从www.baidu.com页面点击链接来这个页面的


###cookies（写爬虫之前一定要考虑）
在scrapy中，
当COOKIES_ENABLED是注释的时候scrapy默认没有开启cookie
当COOKIES_ENABLED没有注释设置为False的时候scrapy默认使用了settings里面的cookie
当COOKIES_ENABLED设置为True的时候scrapy就会把settings的cookie关掉，使用自定义cookie
总结：
如果使用自定义cookie就把COOKIES_ENABLED设置为True
如果使用settings的cookie就把COOKIES_ENABLED设置为False
####使用
如果，网站不需要登录就能爬去信息，可以禁用cookies





##ajax请求
豆瓣爬书https://read.douban.com/category/?kind=100
这个网页的书名，作者等信息都是通过ajax再请求加载出来的。
并且ajax的url为https://read.douban.com/j/kind/,并且请求方式是post


##scrapy post请求
scrapy默认发送的是get请求
发送post请求时需要重载start_requests(self):


##关于解析（P3P482）
一次解析：
some_books = response.xpath('//li[@class="subject-item"]')
二次解析：
加点在li节点内部提取：item['book_name'] = book.xpath('.//h2[@class=""]/a/@title')
不加点从根节点开始提取：item['book_name'] = book.xpath('//h2[@class=""]/a/@title')


###extract_first()
scrapy在经过几次解析之后，限定了匹配的范围，使XPATH只可以匹配到一个元素，然后用extract（）方法提取，结果是一个列表形式（列表里的元素只有一个，是我们所要的）。
我们将extract_first（）方法替换extract（）方法，来获取列表内容的第一个元素（不再是列表，而是里面的元素）
some_books = response.xpath('//li[@class="subject-item"]')
item['book_name'] = book.xpath('.//h2[@class=""]/a/@title').extract()
//获取的元素是列表形式，是一个只包含一个元素的列表

some_books = response.xpath('//li[@class="subject-item"]')
item['book_name'] = book.xpath('.//h2[@class=""]/a/@title').extract_first()
//返回的元素是这个列表中的第一个元素


###解析时元素碰到空格如何解决
1.join()方法+split()方法，可以去除全部空格
# join为字符字符串合成传入一个字符串列表，split用于字符串分割可以按规则进行分割

>>> a = " a b c "
>>> b = a.split()  # 字符串按空格分割成列表
>>> b ['a', 'b', 'c']
>>> c = "".join(b) # 使用一个空字符串合成列表内容生成新的字符串
>>> c 'abc'

# 快捷用法
>>> a = " a b c "
>>> "".join(a.split())
'abc'
又例：
>>> a = 'hello world'
>>> a = ''.join(a.split())
>>> print(a)
helloworld

2.使用正则表达式
>>> a = 'hello world'
>>> import re
>>> strinfo = re.compile()
>>> strinfo = re.compile(' ')

>>> b = strinfo.sub('', a)
>>> print(b)
helloworld

3.使用replace方法
>>> a = 'hello world'
>>> a.replace(' ', '')  //用‘ ’替换‘’
'helloworld'

##scrapy爬取并请求下一页链接
base_url = 'https://book.douban.com'
part_url = response.xpath('//*[@id="subject_list"]/div[2]/span[4]/a/@href').extract_first() //提取下一页链接关键部分或者可能是完整部分
url = base_url+part_url
yield scrapy.Request(url=url, callback=self.parse())


##解析时使用正则表达式
some_books = response.xpath('//li[@class="subject-item"]')
        for book in some_books:
        item['book_author'] = book.xpath('.//div[@class="pub"]/text()').re_first('\s*(\S.*?)\/')
        
//解析时碰到很多换行符和字符串用\s*

注意：
在scrapy中，
不能直接用.re，如item = book.xpath('.').re('\s*(\S.*?)\/')，需要xpath等解析器来过渡
.re（）功能与xpath等解析器类似
.re_first()功能与.xpath（）.extract_first（）等类似


##返回网址响应时，注意回到函数的参数，没有括号
yield scrapy.Request(url=one_url, callback=self.follow_parse)



##scrapy如何用mongodb保存爬去的数据
1.在items.py文件的类中加上collection = 'book'这句话（collection代表mongodb存储的collection名称）
class DoubanItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    collection ='book'         //如果要用mongodb存储，加上这句话
    book_name = scrapy.Field()
    book_author = scrapy.Field()
    book_score = scrapy.Field()
2.在settings.py文件中
#MONGODB
MONGO_URI = 'localhost'
MONGO_DB = 'book'
3.在pipelines文件中。
import pymongo

class MongoPipeline(object):
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DB')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def process_item(self, item, spider):
        name = item.collection
        self.db[name].insert(dict(item))
        return item

    def close_spider(self, spider):
        self.client.close()

  4.在settings文件中，
  # Configure item pipelines
  # See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
  ITEM_PIPELINES = {
    'douban.pipelines.MongoPipeline': 300,
  }
  
  
  ##如何用Robo 3T看mongodb数据
  看那个数据，先建立数据库，然后爬，存储，就能看到了
  
  ###如何用Robo3T查询（重复）
  db.getCollection('集合名').find({'字段名':'字段属性'})
  如：
  db.getCollection('book').find({'book_name':'解忧杂货店'})
  
  
  
  #关于item结构形式和如何处理
  若：
    collection = 'comment'
    user = scrapy.Field()
    book = scrapy.Field()
    score = scrapy.Field()
    date = scrapy.Field()
  则在spider.py文件中parse（）处理方法中，
  print(item),输出item是字典形式，得{'book': '宋徽宗', 'date': '2018-10-28 09:11:29', 'score': 1, 'user': '乐乐'}结构
  print(item.collection)，输出字符串，得comment
  
  在pipelines.py文件process_item（）方法中（注：没有from douban.items import UtiItem），
  print(item),输出item是字典形式,得{'book': '宋徽宗', 'date': '2018-10-28 09:11:29', 'score': 1, 'user': '乐乐'}
 print(item['user'])，输出字符串，得mumudancing
 print(item.user)，输出item.user，
  报错：raise AttributeError("Use item[%r] to get field value" % name)
AttributeError: Use item['user'] to get field value


  
